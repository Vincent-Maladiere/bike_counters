{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from datetime import datetime\n",
    "import holidays\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(df, df_name):\n",
    "    missing_values_count = df.isnull().any(axis=1).sum()\n",
    "    print(f\"({df_name}) : Number of lines with missing values: {missing_values_count}\")\n",
    "    df = df.dropna()\n",
    "    return df\n",
    "\n",
    "def _encode_dates_bike(df):\n",
    "    df[\"year\"] = df[\"date\"].dt.year\n",
    "    df[\"month\"] = df[\"date\"].dt.month\n",
    "    df[\"day\"] = df[\"date\"].dt.day\n",
    "    df[\"hour\"] = df[\"date\"].dt.hour\n",
    "    df[\"weekday\"] = df[\"date\"].dt.weekday\n",
    "    df['IsWeekend'] = df['weekday'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "\n",
    "    vacances = holidays.CountryHoliday('France', years=[i for i in range(2009, 2025)])\n",
    "    vacances_dates = pd.to_datetime(list(vacances.keys())).date\n",
    "    df[\"IsHolidays\"] = df[\"date\"].isin(vacances_dates).astype(int)\n",
    "\n",
    "    return df\n",
    "\n",
    "def _encode_dates_meteo(df, min_date, max_date):   \n",
    "    df = df[(df['DATE'] > min_date) & (df['DATE'] < max_date)]\n",
    "    df['DATE'] = pd.to_datetime(df['DATE'])\n",
    "    df[\"year\"] = df[\"DATE\"].dt.year\n",
    "    df[\"month\"] = df[\"DATE\"].dt.month\n",
    "    df[\"day\"] = df[\"DATE\"].dt.day\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(raw_bike_counter_data, raw_data_meteo, scaler):\n",
    "\n",
    "    bike_counter_data = raw_bike_counter_data.copy()\n",
    "    data_meteo = raw_data_meteo.copy()\n",
    "\n",
    "    bike_counter_data = clean(bike_counter_data, 'bike_counter_data')\n",
    "    bike_counter_data = _encode_dates_bike(bike_counter_data)\n",
    "    #print(bike_counter_data.info())\n",
    "    min_date_bike = bike_counter_data['date'].min().strftime('%Y-%m-%d')\n",
    "    max_date_bike = bike_counter_data['date'].max().strftime('%Y-%m-%d')\n",
    "\n",
    "    data_meteo = data_meteo.drop(columns = ['TEMPERATURE_NIGHT_C', 'SUNRISE', 'SUNSET'] )\n",
    "    data_meteo = clean(data_meteo, 'data_meteo')\n",
    "    data_meteo = _encode_dates_meteo(data_meteo, min_date_bike, max_date_bike)\n",
    "    #print(data_meteo.info())    \n",
    "    merged_data = pd.merge(bike_counter_data, data_meteo, on=['year', 'month', 'day'], how='left')\n",
    "\n",
    "    all_dates = pd.merge(\n",
    "        bike_counter_data[['year', 'month', 'day']],\n",
    "        data_meteo[['year', 'month', 'day']],\n",
    "        on=['year', 'month', 'day'],\n",
    "        how='outer',\n",
    "        indicator=True\n",
    "    )\n",
    "\n",
    "    # Filter for rows that are only in bike_counter_data\n",
    "    missing_dates = all_dates[all_dates['_merge'] == 'left_only']\n",
    "\n",
    "    print(\"Missing dates in data_meteo:\")\n",
    "    print(missing_dates[['year', 'month', 'day']])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    columns_to_drop = [\"day\", \"counter_name\", \"site_name\", \"DATE\", \"counter_installation_date\", \"coordinates\", \"counter_technical_id\", \"latitude\", \"longitude\"]\n",
    "    if 'bike_count' in merged_data.columns:\n",
    "        merged_data = merged_data.drop(columns = 'bike_count')\n",
    "    merged_data = merged_data.drop(columns = columns_to_drop)\n",
    "\n",
    "    columns_to_encode = ['WEATHER_CODE_MORNING', 'WEATHER_CODE_NOON', 'WEATHER_CODE_EVENING', 'OPINION', \"counter_id\", \"date\"]\n",
    "    label_encoders = {}\n",
    "\n",
    "    for col in columns_to_encode:\n",
    "        le = LabelEncoder()\n",
    "        merged_data[col] = le.fit_transform(merged_data[col])\n",
    "        label_encoders[col] = le     # Garder une référence pour un éventuel inverse_transform\n",
    "\n",
    "    # Scale the data\n",
    "    merged_data[merged_data.columns] = scaler.fit_transform(merged_data[merged_data.columns])\n",
    "    print(len(bike_counter_data))\n",
    "    print(len(merged_data))\n",
    "    \n",
    "    return merged_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(bike_counter_data) : Number of lines with missing values: 0\n",
      "(data_meteo) : Number of lines with missing values: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/48/xwb303h13cj206r81_rrqr840000gn/T/ipykernel_48517/2930753384.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['DATE'] = pd.to_datetime(df['DATE'])\n",
      "/var/folders/48/xwb303h13cj206r81_rrqr840000gn/T/ipykernel_48517/2930753384.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"year\"] = df[\"DATE\"].dt.year\n",
      "/var/folders/48/xwb303h13cj206r81_rrqr840000gn/T/ipykernel_48517/2930753384.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"month\"] = df[\"DATE\"].dt.month\n",
      "/var/folders/48/xwb303h13cj206r81_rrqr840000gn/T/ipykernel_48517/2930753384.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"day\"] = df[\"DATE\"].dt.day\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing dates in data_meteo:\n",
      "        year  month  day\n",
      "0       2020      9    1\n",
      "1       2020      9    1\n",
      "2       2020      9    1\n",
      "3       2020      9    1\n",
      "4       2020      9    1\n",
      "...      ...    ...  ...\n",
      "496822  2021      9    9\n",
      "496823  2021      9    9\n",
      "496824  2021      9    9\n",
      "496825  2021      9    9\n",
      "496826  2021      9    9\n",
      "\n",
      "[2586 rows x 3 columns]\n",
      "496827\n",
      "496827\n",
      "(bike_counter_data) : Number of lines with missing values: 0\n",
      "(data_meteo) : Number of lines with missing values: 0\n",
      "Missing dates in data_meteo:\n",
      "       year  month  day\n",
      "0      2021      9   10\n",
      "1      2021      9   10\n",
      "2      2021      9   10\n",
      "3      2021      9   10\n",
      "4      2021      9   10\n",
      "...     ...    ...  ...\n",
      "35921  2021     10   18\n",
      "35922  2021     10   18\n",
      "35923  2021     10   18\n",
      "35924  2021     10   18\n",
      "35925  2021     10   18\n",
      "\n",
      "[2520 rows x 3 columns]\n",
      "51440\n",
      "51440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/48/xwb303h13cj206r81_rrqr840000gn/T/ipykernel_48517/2930753384.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['DATE'] = pd.to_datetime(df['DATE'])\n",
      "/var/folders/48/xwb303h13cj206r81_rrqr840000gn/T/ipykernel_48517/2930753384.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"year\"] = df[\"DATE\"].dt.year\n",
      "/var/folders/48/xwb303h13cj206r81_rrqr840000gn/T/ipykernel_48517/2930753384.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"month\"] = df[\"DATE\"].dt.month\n",
      "/var/folders/48/xwb303h13cj206r81_rrqr840000gn/T/ipykernel_48517/2930753384.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"day\"] = df[\"DATE\"].dt.day\n"
     ]
    }
   ],
   "source": [
    "raw_bike_counter_data = pd.read_parquet(Path(\"data\") / \"train.parquet\")\n",
    "raw_meteo_data = pd.read_csv('external_data/export-paris0.csv')\n",
    "raw_bike_counter_test_data = pd.read_parquet(Path(\"data\") / \"final_test.parquet\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_data = transform_data(raw_bike_counter_data, raw_meteo_data, scaler)\n",
    "public_test_data = transform_data(raw_bike_counter_test_data, raw_meteo_data, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 496827 entries, 0 to 496826\n",
      "Data columns (total 31 columns):\n",
      " #   Column                  Non-Null Count   Dtype  \n",
      "---  ------                  --------------   -----  \n",
      " 0   counter_id              496827 non-null  float64\n",
      " 1   site_id                 496827 non-null  float64\n",
      " 2   date                    496827 non-null  float64\n",
      " 3   log_bike_count          496827 non-null  float64\n",
      " 4   year                    496827 non-null  float64\n",
      " 5   month                   496827 non-null  float64\n",
      " 6   hour                    496827 non-null  float64\n",
      " 7   weekday                 496827 non-null  float64\n",
      " 8   IsWeekend               496827 non-null  float64\n",
      " 9   IsHolidays              496827 non-null  float64\n",
      " 10  MAX_TEMPERATURE_C       494241 non-null  float64\n",
      " 11  MIN_TEMPERATURE_C       494241 non-null  float64\n",
      " 12  WINDSPEED_MAX_KMH       494241 non-null  float64\n",
      " 13  TEMPERATURE_MORNING_C   494241 non-null  float64\n",
      " 14  TEMPERATURE_NOON_C      494241 non-null  float64\n",
      " 15  TEMPERATURE_EVENING_C   494241 non-null  float64\n",
      " 16  PRECIP_TOTAL_DAY_MM     494241 non-null  float64\n",
      " 17  HUMIDITY_MAX_PERCENT    494241 non-null  float64\n",
      " 18  VISIBILITY_AVG_KM       494241 non-null  float64\n",
      " 19  PRESSURE_MAX_MB         494241 non-null  float64\n",
      " 20  CLOUDCOVER_AVG_PERCENT  494241 non-null  float64\n",
      " 21  HEATINDEX_MAX_C         494241 non-null  float64\n",
      " 22  DEWPOINT_MAX_C          494241 non-null  float64\n",
      " 23  WINDTEMP_MAX_C          494241 non-null  float64\n",
      " 24  WEATHER_CODE_MORNING    496827 non-null  float64\n",
      " 25  WEATHER_CODE_NOON       496827 non-null  float64\n",
      " 26  WEATHER_CODE_EVENING    496827 non-null  float64\n",
      " 27  TOTAL_SNOW_MM           494241 non-null  float64\n",
      " 28  UV_INDEX                494241 non-null  float64\n",
      " 29  SUNHOUR                 494241 non-null  float64\n",
      " 30  OPINION                 496827 non-null  float64\n",
      "dtypes: float64(31)\n",
      "memory usage: 121.3 MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 51440 entries, 0 to 51439\n",
      "Data columns (total 30 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   counter_id              51440 non-null  float64\n",
      " 1   site_id                 51440 non-null  float64\n",
      " 2   date                    51440 non-null  float64\n",
      " 3   year                    51440 non-null  float64\n",
      " 4   month                   51440 non-null  float64\n",
      " 5   hour                    51440 non-null  float64\n",
      " 6   weekday                 51440 non-null  float64\n",
      " 7   IsWeekend               51440 non-null  float64\n",
      " 8   IsHolidays              51440 non-null  float64\n",
      " 9   MAX_TEMPERATURE_C       48920 non-null  float64\n",
      " 10  MIN_TEMPERATURE_C       48920 non-null  float64\n",
      " 11  WINDSPEED_MAX_KMH       48920 non-null  float64\n",
      " 12  TEMPERATURE_MORNING_C   48920 non-null  float64\n",
      " 13  TEMPERATURE_NOON_C      48920 non-null  float64\n",
      " 14  TEMPERATURE_EVENING_C   48920 non-null  float64\n",
      " 15  PRECIP_TOTAL_DAY_MM     48920 non-null  float64\n",
      " 16  HUMIDITY_MAX_PERCENT    48920 non-null  float64\n",
      " 17  VISIBILITY_AVG_KM       48920 non-null  float64\n",
      " 18  PRESSURE_MAX_MB         48920 non-null  float64\n",
      " 19  CLOUDCOVER_AVG_PERCENT  48920 non-null  float64\n",
      " 20  HEATINDEX_MAX_C         48920 non-null  float64\n",
      " 21  DEWPOINT_MAX_C          48920 non-null  float64\n",
      " 22  WINDTEMP_MAX_C          48920 non-null  float64\n",
      " 23  WEATHER_CODE_MORNING    51440 non-null  float64\n",
      " 24  WEATHER_CODE_NOON       51440 non-null  float64\n",
      " 25  WEATHER_CODE_EVENING    51440 non-null  float64\n",
      " 26  TOTAL_SNOW_MM           48920 non-null  float64\n",
      " 27  UV_INDEX                48920 non-null  float64\n",
      " 28  SUNHOUR                 48920 non-null  float64\n",
      " 29  OPINION                 51440 non-null  float64\n",
      "dtypes: float64(30)\n",
      "memory usage: 12.2 MB\n"
     ]
    }
   ],
   "source": [
    "train_data.info()\n",
    "public_test_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import ExtraTreesRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = train_data[\"log_bike_count\"]\n",
    "data = train_data.drop(columns = \"log_bike_count\")\n",
    "\n",
    "X_train_sample = data.sample(frac=0.01, random_state=42)  # x % des données\n",
    "y_train_sample = target.loc[X_train_sample.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 216 candidates, totalling 1080 fits\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\nAll the 1080 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n1080 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/Users/gustavetriomphe/anaconda3/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/Users/gustavetriomphe/anaconda3/lib/python3.10/site-packages/sklearn/pipeline.py\", line 405, in fit\n    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n  File \"/Users/gustavetriomphe/anaconda3/lib/python3.10/site-packages/sklearn/ensemble/_forest.py\", line 345, in fit\n    X, y = self._validate_data(\n  File \"/Users/gustavetriomphe/anaconda3/lib/python3.10/site-packages/sklearn/base.py\", line 565, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n  File \"/Users/gustavetriomphe/anaconda3/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 1106, in check_X_y\n    X = check_array(\n  File \"/Users/gustavetriomphe/anaconda3/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 921, in check_array\n    _assert_all_finite(\n  File \"/Users/gustavetriomphe/anaconda3/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 161, in _assert_all_finite\n    raise ValueError(msg_err)\nValueError: Input X contains NaN.\nExtraTreesRegressor does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[103], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(pipeline, param_grid, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Fit the model\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m grid_search_result \u001b[38;5;241m=\u001b[39m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_sample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_sample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Output the best results\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest hyperparameters found: \u001b[39m\u001b[38;5;124m\"\u001b[39m, grid_search\u001b[38;5;241m.\u001b[39mbest_params_)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/sklearn/model_selection/_search.py:874\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    868\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m    869\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    870\u001b[0m     )\n\u001b[1;32m    872\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m--> 874\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    878\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/sklearn/model_selection/_search.py:1388\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1386\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1387\u001b[0m     \u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1388\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/sklearn/model_selection/_search.py:851\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    844\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m!=\u001b[39m n_candidates \u001b[38;5;241m*\u001b[39m n_splits:\n\u001b[1;32m    845\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    846\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcv.split and cv.get_n_splits returned \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    847\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minconsistent results. Expected \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    848\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplits, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_splits, \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m n_candidates)\n\u001b[1;32m    849\u001b[0m     )\n\u001b[0;32m--> 851\u001b[0m \u001b[43m_warn_or_raise_about_fit_failures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[38;5;66;03m# For callable self.scoring, the return type is only know after\u001b[39;00m\n\u001b[1;32m    854\u001b[0m \u001b[38;5;66;03m# calling. If the return type is a dictionary, the error scores\u001b[39;00m\n\u001b[1;32m    855\u001b[0m \u001b[38;5;66;03m# can now be inserted with the correct key. The type checking\u001b[39;00m\n\u001b[1;32m    856\u001b[0m \u001b[38;5;66;03m# of out will be done in `_insert_error_scores`.\u001b[39;00m\n\u001b[1;32m    857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m callable(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscoring):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:367\u001b[0m, in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[0;34m(results, error_score)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_failed_fits \u001b[38;5;241m==\u001b[39m num_fits:\n\u001b[1;32m    361\u001b[0m     all_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    362\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAll the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    363\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt is very likely that your model is misconfigured.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    364\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can try to debug the error by setting error_score=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    365\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    366\u001b[0m     )\n\u001b[0;32m--> 367\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    370\u001b[0m     some_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    371\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mnum_failed_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed out of a total of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    372\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe score on these train-test partitions for these parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    377\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: \nAll the 1080 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n1080 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/Users/gustavetriomphe/anaconda3/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/Users/gustavetriomphe/anaconda3/lib/python3.10/site-packages/sklearn/pipeline.py\", line 405, in fit\n    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n  File \"/Users/gustavetriomphe/anaconda3/lib/python3.10/site-packages/sklearn/ensemble/_forest.py\", line 345, in fit\n    X, y = self._validate_data(\n  File \"/Users/gustavetriomphe/anaconda3/lib/python3.10/site-packages/sklearn/base.py\", line 565, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n  File \"/Users/gustavetriomphe/anaconda3/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 1106, in check_X_y\n    X = check_array(\n  File \"/Users/gustavetriomphe/anaconda3/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 921, in check_array\n    _assert_all_finite(\n  File \"/Users/gustavetriomphe/anaconda3/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 161, in _assert_all_finite\n    raise ValueError(msg_err)\nValueError: Input X contains NaN.\nExtraTreesRegressor does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n"
     ]
    }
   ],
   "source": [
    "# Define the regressor\n",
    "reg = ExtraTreesRegressor(random_state=42, n_jobs=-1)\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    #('preprocessor', preprocessor),\n",
    "    ('regressor', reg)\n",
    "])\n",
    "\n",
    "# Update param_grid to toggle scalers within the 'num' pipeline\n",
    "param_grid = {\n",
    "    #'preprocessor__scaled_num__scaler': [StandardScaler(), MinMaxScaler(), 'passthrough'],  # Apply scalers or skip\n",
    "    'regressor__n_estimators': [50, 100, 200],\n",
    "    'regressor__max_depth': [3, 5, 7],\n",
    "    'regressor__min_samples_split': [2, 5],\n",
    "    'regressor__min_samples_leaf': [1, 2],\n",
    "    'regressor__max_features': ['sqrt', 'log2', None],\n",
    "    'regressor__bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, n_jobs=-1, verbose=1)\n",
    "\n",
    "# Fit the model\n",
    "grid_search_result = grid_search.fit(X_train_sample, y_train_sample)\n",
    "\n",
    "# Output the best results\n",
    "print(\"Best hyperparameters found: \", grid_search.best_params_)\n",
    "print(\"Best cross-validation score: \", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_output(grid_search_result, test_data):\n",
    "    y_pred = grid_search_result.predict(test_data)\n",
    "    results = pd.DataFrame(\n",
    "        dict(\n",
    "            Id=np.arange(y_pred.shape[0]),\n",
    "            log_bike_count=y_pred,\n",
    "        )\n",
    "    )\n",
    "    results.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output(grid_search_result, public_test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
