{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from vacances_scolaires_france import SchoolHolidayDates\n",
    "from jours_feries_france import JoursFeries\n",
    "from code2 import date_encoder, prepare_data, build_pipeline, tune_hyperparameters, evaluate_model, train_model, test_model_kaggle, fit_encoder, encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_parquet(\"/Users/solalzana/Desktop/X/Python for Data Science/Final Project/bike_counters/data/train.parquet\")\n",
    "df_test = pd.read_parquet(\"/Users/solalzana/Desktop/X/Python for Data Science/Final Project/bike_counters/data/final_test.parquet\")\n",
    "df_ext = pd.read_csv(\"/Users/solalzana/Desktop/X/Python for Data Science/Final Project/bike_counters/data/external_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_cleaned = prepare_data(df_train, df_ext)\n",
    "df_test_cleaned = prepare_data(df_test, df_ext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train_cleaned.drop(columns=[\"log_bike_count\", \"bike_count\"])\n",
    "X_test = df_test_cleaned\n",
    "y_train = df_train_cleaned['log_bike_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(df_train_cleaned.drop(columns=[\"log_bike_count\", \"bike_count\"]), df_train_cleaned['log_bike_count'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import PCA\n",
    "\n",
    "# pca = PCA()\n",
    "# pca.fit(df_ext)\n",
    "\n",
    "# # Calculate the cumulative explained variance ratio\n",
    "# cumulative_explained_variance = pca.explained_variance_ratio_.cumsum()\n",
    "\n",
    "# # Plot the cumulative explained variance ratio\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.plot(\n",
    "#     range(1, len(cumulative_explained_variance) + 1),\n",
    "#     cumulative_explained_variance,\n",
    "#     marker=\"o\",\n",
    "#     linestyle=\"--\",\n",
    "#     color=\"b\",\n",
    "# )\n",
    "# plt.title(\"Explained Variance depending on the number of parameters\")\n",
    "# plt.xlabel(\"Number of Parameters\")\n",
    "# plt.ylabel(\" Explained Variance\")\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### build pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file created, check data folder\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(          Id  log_bike_count\n",
       " 0          0        0.505692\n",
       " 1          1        1.234730\n",
       " 2          2        2.224302\n",
       " 3          3        0.774954\n",
       " 4          4        0.203119\n",
       " ...      ...             ...\n",
       " 51435  51435        4.278306\n",
       " 51436  51436        4.715961\n",
       " 51437  51437        4.850592\n",
       " 51438  51438        4.732472\n",
       " 51439  51439        3.776109\n",
       " \n",
       " [51440 rows x 2 columns],\n",
       " None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "import xgboost as xgb\n",
    "\n",
    "fit_encoder(X_train)\n",
    "# X_train = encoder(X_train)\n",
    "# X_test = encoder(X_test)\n",
    "\n",
    "model = xgb.XGBRegressor(objective='reg:squarederror')\n",
    "trained_model = train_model(X_train, y_train, model)\n",
    "\n",
    "test_model_kaggle(trained_model, X_test, \"xgb\") # results is a df storing y_pred(s)\n",
    "# check submission folder now\n",
    "# X_test.drop(columns=['date'], inplace=True)\n",
    "# evaluate_model(trained_model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preprocessor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcatboost\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CatBoostRegressor\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m CatBoostRegressor()\n\u001b[0;32m----> 4\u001b[0m pipeline_cb \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m trained_model_cb \u001b[38;5;241m=\u001b[39m train_model(pipeline_cb, model, X_train, y_train)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# test_model_kaggle(pipeline_cb, X_test, \"cb\") # results is a df storing y_pred(s)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# # check submission folder now\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/X/Python for Data Science/Final Project/bike_counters/Code/code2.py:203\u001b[0m, in \u001b[0;36mbuild_pipeline\u001b[0;34m(X_train, y_train, model)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Builds a preprocessing and modeling pipeline.\"\"\"\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;66;03m# num_transformer = StandardScaler()\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;66;03m# cat_transformer = OneHotEncoder(handle_unknown=\"ignore\")\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;66;03m#     ]\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[0;32m--> 203\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m make_pipeline(steps\u001b[38;5;241m=\u001b[39m[(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreprocessor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mpreprocessor\u001b[49m), (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, model)])\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pipeline\n",
      "\u001b[0;31mNameError\u001b[0m: name 'preprocessor' is not defined"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "\n",
    "model = CatBoostRegressor()\n",
    "pipeline_cb = build_pipeline(X_train, y_train, model)\n",
    "trained_model_cb = train_model(pipeline_cb, model, X_train, y_train)\n",
    "\n",
    "# test_model_kaggle(pipeline_cb, X_test, \"cb\") # results is a df storing y_pred(s)\n",
    "# # check submission folder now\n",
    "test_model_kaggle(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show all rows\n",
    "X_train[X_train[\"quarantine1\"]==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lightgbm\n",
    "!pip install lightgbm\n",
    "from lightgbm import LGBMRegressor\n",
    "import lightgbm as lgb\n",
    "\n",
    "model = lgb.LGBMRegressor()\n",
    "pipeline_lgb = build_pipeline(X_train, y_train, model)\n",
    "trained_model_lgb = train_model(pipeline_lgb, model, X_train, y_train)\n",
    "\n",
    "test_model_kaggle(pipeline_lgb, X_test, \"lgb\") # results is a df storing y_pred(s)\n",
    "# check submission folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "model = RandomForestRegressor(n_jobs=-1)\n",
    "pipeline_cb = build_pipeline(X_train, y_train, model)\n",
    "trained_model_cb = train_model(pipeline_cb, model, X_train, y_train)\n",
    "\n",
    "test_model_kaggle(pipeline_xgb, X_test, \"rf\") # results is a df storing y_pred(s)\n",
    "# check submission folder now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  tune hyperPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_best_model = tune_hyperparameters(pipeline_xgb, X_train, y_train).best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_best = pipeline_best_model.fit(X_train, y_train)\n",
    "y_pred = test_model_kaggle(fitted_best, X_test, \"xgb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.XGBRegressor(objective='reg:squarederror')\n",
    "y_pred_best = train_model(pipeline_best_model, \"xgb\", X_train, y_train) # y predictions to be submitted to Kaggle\n",
    "results = test_model_kaggle(pipeline_best_model, X_test, \"xgb\") # results is a df storing y_pred(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_xgb_best = build_pipeline(X_train, y_train, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### evaluate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(best_model, X_test, y_test) # returns rmse with the best parameters\n",
    "# function when train/test splitting on the train set only, not the Kaggle test set"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
