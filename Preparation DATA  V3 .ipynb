{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Python final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A) DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/train.parquet'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgeopandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgpd\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mshapely\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeometry\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Point\n\u001b[0;32m---> 12\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain.parquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# import train \u001b[39;00m\n\u001b[1;32m     14\u001b[0m meteo \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(Path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexternal_data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m# import meteo \u001b[39;00m\n\u001b[1;32m     16\u001b[0m meteo\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[0;32m~/bike_counters-AM-/env/lib/python3.8/site-packages/pandas/io/parquet.py:509\u001b[0m, in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, **kwargs)\u001b[0m\n\u001b[1;32m    506\u001b[0m     use_nullable_dtypes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    507\u001b[0m check_dtype_backend(dtype_backend)\n\u001b[0;32m--> 509\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    511\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    512\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    513\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/bike_counters-AM-/env/lib/python3.8/site-packages/pandas/io/parquet.py:220\u001b[0m, in \u001b[0;36mPyArrowImpl.read\u001b[0;34m(self, path, columns, use_nullable_dtypes, dtype_backend, storage_options, **kwargs)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m manager \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    218\u001b[0m     to_pandas_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit_blocks\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[0;32m--> 220\u001b[0m path_or_handle, handles, kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilesystem\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43m_get_path_or_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfilesystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    227\u001b[0m     pa_table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mparquet\u001b[38;5;241m.\u001b[39mread_table(\n\u001b[1;32m    228\u001b[0m         path_or_handle, columns\u001b[38;5;241m=\u001b[39mcolumns, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    229\u001b[0m     )\n",
      "File \u001b[0;32m~/bike_counters-AM-/env/lib/python3.8/site-packages/pandas/io/parquet.py:110\u001b[0m, in \u001b[0;36m_get_path_or_handle\u001b[0;34m(path, fs, storage_options, mode, is_dir)\u001b[0m\n\u001b[1;32m    100\u001b[0m handles \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m fs\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dir\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;66;03m# fsspec resources can also point to directories\u001b[39;00m\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;66;03m# this branch is used for example when reading from non-fsspec URLs\u001b[39;00m\n\u001b[0;32m--> 110\u001b[0m     handles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m     fs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    114\u001b[0m     path_or_handle \u001b[38;5;241m=\u001b[39m handles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/bike_counters-AM-/env/lib/python3.8/site-packages/pandas/io/common.py:868\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    860\u001b[0m             handle,\n\u001b[1;32m    861\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    864\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    865\u001b[0m         )\n\u001b[1;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m--> 868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    869\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[1;32m    871\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/train.parquet'"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import holidays\n",
    "import osmnx as ox\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "data = pd.read_parquet(Path(\"data\") / \"train.parquet\") # import train \n",
    "\n",
    "meteo = pd.read_csv(Path(\"data\") / \"external_data.csv\") # import meteo \n",
    "\n",
    "meteo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3322 entries, 0 to 3321\n",
      "Data columns (total 59 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   numer_sta  3322 non-null   int64  \n",
      " 1   date       3322 non-null   object \n",
      " 2   pmer       3322 non-null   int64  \n",
      " 3   tend       3322 non-null   int64  \n",
      " 4   cod_tend   3322 non-null   int64  \n",
      " 5   dd         3322 non-null   int64  \n",
      " 6   ff         3322 non-null   float64\n",
      " 7   t          3322 non-null   float64\n",
      " 8   td         3322 non-null   float64\n",
      " 9   u          3322 non-null   int64  \n",
      " 10  vv         3322 non-null   int64  \n",
      " 11  ww         3322 non-null   int64  \n",
      " 12  w1         3315 non-null   float64\n",
      " 13  w2         3312 non-null   float64\n",
      " 14  n          3166 non-null   float64\n",
      " 15  nbas       3317 non-null   float64\n",
      " 16  hbas       2869 non-null   float64\n",
      " 17  cl         2909 non-null   float64\n",
      " 18  cm         1941 non-null   float64\n",
      " 19  ch         1678 non-null   float64\n",
      " 20  pres       3322 non-null   int64  \n",
      " 21  niv_bar    0 non-null      float64\n",
      " 22  geop       0 non-null      float64\n",
      " 23  tend24     3312 non-null   float64\n",
      " 24  tn12       830 non-null    float64\n",
      " 25  tn24       0 non-null      float64\n",
      " 26  tx12       830 non-null    float64\n",
      " 27  tx24       0 non-null      float64\n",
      " 28  tminsol    1 non-null      float64\n",
      " 29  sw         0 non-null      float64\n",
      " 30  tw         0 non-null      float64\n",
      " 31  raf10      3312 non-null   float64\n",
      " 32  rafper     3322 non-null   float64\n",
      " 33  per        3322 non-null   int64  \n",
      " 34  etat_sol   3270 non-null   float64\n",
      " 35  ht_neige   3273 non-null   float64\n",
      " 36  ssfrai     2877 non-null   float64\n",
      " 37  perssfrai  2877 non-null   float64\n",
      " 38  rr1        3313 non-null   float64\n",
      " 39  rr3        3316 non-null   float64\n",
      " 40  rr6        3306 non-null   float64\n",
      " 41  rr12       3300 non-null   float64\n",
      " 42  rr24       3298 non-null   float64\n",
      " 43  phenspe1   0 non-null      float64\n",
      " 44  phenspe2   0 non-null      float64\n",
      " 45  phenspe3   0 non-null      float64\n",
      " 46  phenspe4   0 non-null      float64\n",
      " 47  nnuage1    2873 non-null   float64\n",
      " 48  ctype1     2524 non-null   float64\n",
      " 49  hnuage1    2867 non-null   float64\n",
      " 50  nnuage2    1695 non-null   float64\n",
      " 51  ctype2     1443 non-null   float64\n",
      " 52  hnuage2    1695 non-null   float64\n",
      " 53  nnuage3    618 non-null    float64\n",
      " 54  ctype3     470 non-null    float64\n",
      " 55  hnuage3    618 non-null    float64\n",
      " 56  nnuage4    42 non-null     float64\n",
      " 57  ctype4     87 non-null     float64\n",
      " 58  hnuage4    42 non-null     float64\n",
      "dtypes: float64(48), int64(10), object(1)\n",
      "memory usage: 1.5+ MB\n"
     ]
    }
   ],
   "source": [
    "meteo.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A-1) DONNEE METEO   \n",
    "ETAPE 1: Filtrer les données (date et geographie et NaN)\n",
    "\n",
    "1/ FILTRER PAR DATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meteo dataset: 2021-10-21 12:00:00 2020-09-01 00:00:00\n",
      "Data dataset: 2021-09-09 23:00:00 2020-09-01 01:00:00\n"
     ]
    }
   ],
   "source": [
    "# DATE \n",
    "\n",
    "# Convertir les colonne en type datetime\n",
    "meteo['date'] = pd.to_datetime(meteo['date'], format='%Y-%m-%d %H:%M:%S')\n",
    "data['date'] = pd.to_datetime(data['date'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Trouver la date minimum et maximum de meteo et data\n",
    "date_min_meteo = meteo['date'].min()\n",
    "date_max_meteo = meteo['date'].max()\n",
    "date_min_data = data['date'].min()\n",
    "date_max_data = data['date'].max()\n",
    "\n",
    "print(\"Meteo dataset:\", date_max_meteo, date_min_meteo)\n",
    "print(\"Data dataset:\", date_max_data, date_min_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Donc, ces données overlap nos training, mais sont dans le meme format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtrer\n",
    "meteo = meteo[(meteo['date'] >= date_min_data) & (meteo['date'] <= date_max_data)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2/ FILTRER PAR GEOGRAPHIE \n",
    "\n",
    "On remarque qu'il n'y a aucune indication sur la geographie (region, departement, commune, longitude ou latitude)\n",
    "DONC, on va HYPOTHESE IMPORTANTE: supposer que toutes les données concernent paris intra-muros (ou sont localisé nos bornes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3/ GERER LES VALEURS MANQUANTES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lignes: 2988 colonnes: 59\n"
     ]
    }
   ],
   "source": [
    "# Gerer les valeurs manquantes\n",
    "print(\"Lignes:\", meteo.shape[0], \"colonnes:\", meteo.shape[1]) # trouver le nombre de lignes dans le dataset filtré"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numer_sta       0\n",
       "date            0\n",
       "pmer            0\n",
       "tend            0\n",
       "cod_tend        0\n",
       "dd              0\n",
       "ff              0\n",
       "t               0\n",
       "td              0\n",
       "u               0\n",
       "vv              0\n",
       "ww              0\n",
       "w1              7\n",
       "w2             10\n",
       "n             138\n",
       "nbas            5\n",
       "hbas          410\n",
       "cl            371\n",
       "cm           1284\n",
       "ch           1526\n",
       "pres            0\n",
       "niv_bar      2988\n",
       "geop         2988\n",
       "tend24         10\n",
       "tn12         2241\n",
       "tn24         2988\n",
       "tx12         2241\n",
       "tx24         2988\n",
       "tminsol      2987\n",
       "sw           2988\n",
       "tw           2988\n",
       "raf10          10\n",
       "rafper          0\n",
       "per             0\n",
       "etat_sol       49\n",
       "ht_neige       44\n",
       "ssfrai        403\n",
       "perssfrai     403\n",
       "rr1             8\n",
       "rr3             4\n",
       "rr6            12\n",
       "rr12           16\n",
       "rr24           18\n",
       "phenspe1     2988\n",
       "phenspe2     2988\n",
       "phenspe3     2988\n",
       "phenspe4     2988\n",
       "nnuage1       406\n",
       "ctype1        720\n",
       "hnuage1       412\n",
       "nnuage2      1466\n",
       "ctype2       1691\n",
       "hnuage2      1466\n",
       "nnuage3      2445\n",
       "ctype3       2577\n",
       "hnuage3      2445\n",
       "nnuage4      2954\n",
       "ctype4       2914\n",
       "hnuage4      2954\n",
       "dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meteo.isna().sum() # Savoir le nombre de Nan dans chaque colonnes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lignes: 2988 colonnes: 49\n",
      "Lignes: 2988 colonnes: 48\n"
     ]
    }
   ],
   "source": [
    "# Ensuite, on retire toutes les colonnes qui sont remplis en totalité de valeurs manquantes\n",
    "meteo = meteo.dropna(axis=1, how='all').copy()\n",
    "print(\"Lignes:\", meteo.shape[0], \"colonnes:\", meteo.shape[1]) # trouver le nombre de lignes dans le dataset filtré\n",
    "\n",
    "# On a egalement remarque que dabs 'Tminsol' il n'y a qu'une valeur, on peut donc egalement supprimer cette colonne\n",
    "meteo = meteo.drop('tminsol', axis=1).copy()\n",
    "print(\"Lignes:\", meteo.shape[0], \"colonnes:\", meteo.shape[1]) # check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4/ GERER LES COLONNES A MODALITE UNIQUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numer_sta       1\n",
       "date         2987\n",
       "pmer          461\n",
       "tend           97\n",
       "cod_tend        9\n",
       "dd             37\n",
       "ff            106\n",
       "t             345\n",
       "td            277\n",
       "u              77\n",
       "vv           1137\n",
       "ww             42\n",
       "w1              9\n",
       "w2              9\n",
       "n              10\n",
       "nbas           10\n",
       "hbas           91\n",
       "cl             11\n",
       "cm              9\n",
       "ch             10\n",
       "pres          458\n",
       "tend24        340\n",
       "tn12          215\n",
       "tx12          276\n",
       "raf10         168\n",
       "rafper        182\n",
       "per             1\n",
       "etat_sol        9\n",
       "ht_neige        7\n",
       "ssfrai          3\n",
       "perssfrai       2\n",
       "rr1            31\n",
       "rr3            58\n",
       "rr6            84\n",
       "rr12          118\n",
       "rr24          152\n",
       "nnuage1         9\n",
       "ctype1          8\n",
       "hnuage1       270\n",
       "nnuage2         8\n",
       "ctype2          8\n",
       "hnuage2       235\n",
       "nnuage3         8\n",
       "ctype3          8\n",
       "hnuage3       152\n",
       "nnuage4         7\n",
       "ctype4          5\n",
       "hnuage4        24\n",
       "dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meteo.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2988 entries, 0 to 3321\n",
      "Data columns (total 46 columns):\n",
      " #   Column     Non-Null Count  Dtype         \n",
      "---  ------     --------------  -----         \n",
      " 0   date       2988 non-null   datetime64[ns]\n",
      " 1   pmer       2988 non-null   int64         \n",
      " 2   tend       2988 non-null   int64         \n",
      " 3   cod_tend   2988 non-null   int64         \n",
      " 4   dd         2988 non-null   int64         \n",
      " 5   ff         2988 non-null   float64       \n",
      " 6   t          2988 non-null   float64       \n",
      " 7   td         2988 non-null   float64       \n",
      " 8   u          2988 non-null   int64         \n",
      " 9   vv         2988 non-null   int64         \n",
      " 10  ww         2988 non-null   int64         \n",
      " 11  w1         2981 non-null   float64       \n",
      " 12  w2         2978 non-null   float64       \n",
      " 13  n          2850 non-null   float64       \n",
      " 14  nbas       2983 non-null   float64       \n",
      " 15  hbas       2578 non-null   float64       \n",
      " 16  cl         2617 non-null   float64       \n",
      " 17  cm         1704 non-null   float64       \n",
      " 18  ch         1462 non-null   float64       \n",
      " 19  pres       2988 non-null   int64         \n",
      " 20  tend24     2978 non-null   float64       \n",
      " 21  tn12       747 non-null    float64       \n",
      " 22  tx12       747 non-null    float64       \n",
      " 23  raf10      2978 non-null   float64       \n",
      " 24  rafper     2988 non-null   float64       \n",
      " 25  etat_sol   2939 non-null   float64       \n",
      " 26  ht_neige   2944 non-null   float64       \n",
      " 27  ssfrai     2585 non-null   float64       \n",
      " 28  perssfrai  2585 non-null   float64       \n",
      " 29  rr1        2980 non-null   float64       \n",
      " 30  rr3        2984 non-null   float64       \n",
      " 31  rr6        2976 non-null   float64       \n",
      " 32  rr12       2972 non-null   float64       \n",
      " 33  rr24       2970 non-null   float64       \n",
      " 34  nnuage1    2582 non-null   float64       \n",
      " 35  ctype1     2268 non-null   float64       \n",
      " 36  hnuage1    2576 non-null   float64       \n",
      " 37  nnuage2    1522 non-null   float64       \n",
      " 38  ctype2     1297 non-null   float64       \n",
      " 39  hnuage2    1522 non-null   float64       \n",
      " 40  nnuage3    543 non-null    float64       \n",
      " 41  ctype3     411 non-null    float64       \n",
      " 42  hnuage3    543 non-null    float64       \n",
      " 43  nnuage4    34 non-null     float64       \n",
      " 44  ctype4     74 non-null     float64       \n",
      " 45  hnuage4    34 non-null     float64       \n",
      "dtypes: datetime64[ns](1), float64(37), int64(8)\n",
      "memory usage: 1.1 MB\n"
     ]
    }
   ],
   "source": [
    "meteo = meteo.loc[:, meteo.nunique() > 1] # supprimer colonnes à modalité unique\n",
    "meteo.info() # check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5/ GERER LES DUPLICATS\n",
    "\n",
    "On remarque que le dataset a 2988 observations, hors il n'y a que 2987 modalités pour la date. Cela signifie qu'il y a un doublon. Verifions cela. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    date    pmer  tend  cod_tend   dd   ff       t      td  \\\n",
      "2017 2020-11-20 18:00:00  103650    20         2  180  1.0  278.15  276.45   \n",
      "2018 2020-11-20 18:00:00  103650    20         2  180  1.0  278.15  276.45   \n",
      "\n",
      "       u     vv  ...  hnuage1  nnuage2  ctype2  hnuage2  nnuage3  ctype3  \\\n",
      "2017  89  25000  ...   7000.0      NaN     NaN      NaN      NaN     NaN   \n",
      "2018  89  25000  ...   7000.0      NaN     NaN      NaN      NaN     NaN   \n",
      "\n",
      "      hnuage3  nnuage4  ctype4  hnuage4  \n",
      "2017      NaN      NaN     NaN      NaN  \n",
      "2018      NaN      NaN     NaN      NaN  \n",
      "\n",
      "[2 rows x 46 columns]\n"
     ]
    }
   ],
   "source": [
    "duplicates = meteo[meteo['date'].duplicated(keep=False)]  # selectionner le doublons\n",
    "print(duplicates) # Afficher les doublons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perfectly identical observations?  True\n"
     ]
    }
   ],
   "source": [
    "# Il y a donc bien un doublon concernant la date. Verifions si ce sont des doublons \"parfaits\"\n",
    "identiques = duplicates.iloc[0].equals(duplicates.iloc[1]) # verifier l'egalité element par element\n",
    "print(\"Perfectly identical observations? \", identiques) # resultats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On peut donc en supprimer un \n",
    "meteo = meteo.drop_duplicates(subset=['date'], keep='first') # garder que la 1ere occurence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ETAPE 2: Comprendre les données et les grouper en thèmes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2987 entries, 0 to 3321\n",
      "Data columns (total 46 columns):\n",
      " #   Column     Non-Null Count  Dtype         \n",
      "---  ------     --------------  -----         \n",
      " 0   date       2987 non-null   datetime64[ns]\n",
      " 1   pmer       2987 non-null   int64         \n",
      " 2   tend       2987 non-null   int64         \n",
      " 3   cod_tend   2987 non-null   int64         \n",
      " 4   dd         2987 non-null   int64         \n",
      " 5   ff         2987 non-null   float64       \n",
      " 6   t          2987 non-null   float64       \n",
      " 7   td         2987 non-null   float64       \n",
      " 8   u          2987 non-null   int64         \n",
      " 9   vv         2987 non-null   int64         \n",
      " 10  ww         2987 non-null   int64         \n",
      " 11  w1         2980 non-null   float64       \n",
      " 12  w2         2977 non-null   float64       \n",
      " 13  n          2849 non-null   float64       \n",
      " 14  nbas       2982 non-null   float64       \n",
      " 15  hbas       2577 non-null   float64       \n",
      " 16  cl         2616 non-null   float64       \n",
      " 17  cm         1703 non-null   float64       \n",
      " 18  ch         1461 non-null   float64       \n",
      " 19  pres       2987 non-null   int64         \n",
      " 20  tend24     2977 non-null   float64       \n",
      " 21  tn12       746 non-null    float64       \n",
      " 22  tx12       746 non-null    float64       \n",
      " 23  raf10      2977 non-null   float64       \n",
      " 24  rafper     2987 non-null   float64       \n",
      " 25  etat_sol   2938 non-null   float64       \n",
      " 26  ht_neige   2943 non-null   float64       \n",
      " 27  ssfrai     2584 non-null   float64       \n",
      " 28  perssfrai  2584 non-null   float64       \n",
      " 29  rr1        2979 non-null   float64       \n",
      " 30  rr3        2983 non-null   float64       \n",
      " 31  rr6        2975 non-null   float64       \n",
      " 32  rr12       2971 non-null   float64       \n",
      " 33  rr24       2969 non-null   float64       \n",
      " 34  nnuage1    2581 non-null   float64       \n",
      " 35  ctype1     2267 non-null   float64       \n",
      " 36  hnuage1    2575 non-null   float64       \n",
      " 37  nnuage2    1522 non-null   float64       \n",
      " 38  ctype2     1297 non-null   float64       \n",
      " 39  hnuage2    1522 non-null   float64       \n",
      " 40  nnuage3    543 non-null    float64       \n",
      " 41  ctype3     411 non-null    float64       \n",
      " 42  hnuage3    543 non-null    float64       \n",
      " 43  nnuage4    34 non-null     float64       \n",
      " 44  ctype4     74 non-null     float64       \n",
      " 45  hnuage4    34 non-null     float64       \n",
      "dtypes: datetime64[ns](1), float64(37), int64(8)\n",
      "memory usage: 1.1 MB\n"
     ]
    }
   ],
   "source": [
    "meteo.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va creer des groupes de données par thèmes (etant donné que la variable \"Date\" est celle qui va nous permettre de faire le lien avec notre training dataset on la mets partout)\n",
    "1) Pression\n",
    "2) Nuage (et visibilité)\n",
    "3) Precipitations\n",
    "4) Vent\n",
    "5) Temperature\n",
    "6) Neige\n",
    "7) Humidité\n",
    "8) Etat du sol (1 donnée)\n",
    "9) Autre (temps present, temps passé 1 et 2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation des 9 sous-ensembles\n",
    "\n",
    "# 1) Pression\n",
    "pression_columns = ['date', 'pmer', 'tend', 'pres', 'tend24', 'cod_tend']  \n",
    "pression  = meteo[pression_columns]\n",
    "\n",
    "# 2) Nuage (et visibilité)\n",
    "nuage_columns = ['date', 'vv', 'nbas', 'hbas', 'cl', 'cm', 'ch', 'n', 'nnuage1', 'ctype1',  'hnuage1', 'nnuage2', 'ctype2', 'hnuage2', 'nnuage3', 'ctype3', 'hnuage3', 'nnuage4', 'ctype4', 'hnuage4'] \n",
    "nuage = meteo[nuage_columns]\n",
    "\n",
    "# 3) Précipitations\n",
    "precip_columns = ['date', 'rr1', 'rr12', 'rr3', 'rr6', 'rr24']  \n",
    "precip = meteo[precip_columns]\n",
    "\n",
    "# 4) Vent\n",
    "vent_columns = ['date', 'dd', 'ff', 'rafper', 'raf10']  \n",
    "vent = meteo[vent_columns]\n",
    "\n",
    "# 5) Température\n",
    "temp_columns = ['date', 't', 'tn12', 'tx12']  \n",
    "temp = meteo[temp_columns]\n",
    "\n",
    "# 6) Neige\n",
    "neige_columns = ['date', 'ssfrai', 'perssfrai', 'ht_neige']  \n",
    "neige = meteo[neige_columns]\n",
    "\n",
    "# 7) Humidité\n",
    "humidite_columns = ['date', 'u', 'td'] \n",
    "humidite = meteo[humidite_columns]\n",
    "\n",
    "# 8) Etat du sol (1 donnée)\n",
    "etat_sol_columns = ['date', 'etat_sol'] \n",
    "sol = meteo[etat_sol_columns]\n",
    "\n",
    "# 9) Autre (temps présent, temps passé 1 et 2)\n",
    "autre_columns = ['date', 'ww', 'w1', 'w2']  # Autres informations liées au temps passé et présent\n",
    "autre = meteo[autre_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "# Nombre de colonnes dans le dataset initial en excluant la colonne 'date'\n",
    "initial_columns = len(meteo.columns) - 1  # Exclure 'date'\n",
    "\n",
    "# Nombre total de colonnes dans les sous-ensembles\n",
    "total_subsets_columns = (\n",
    "    len(pression_columns) + \n",
    "    len(nuage_columns) + \n",
    "    len(precip_columns) + \n",
    "    len(vent_columns) + \n",
    "    len(temp_columns) + \n",
    "    len(neige_columns) + \n",
    "    len(humidite_columns) + \n",
    "    len(etat_sol_columns) + \n",
    "    len(autre_columns) - 9  # Exclure la colonne 'date' dans chaque sous-ensemble\n",
    ")\n",
    "\n",
    "# Vérification si la somme des colonnes dans les sous-ensembles est égale au nombre de colonnes initial\n",
    "if initial_columns == total_subsets_columns:\n",
    "    print(\"OK\")\n",
    "else:\n",
    "    print(\"Problem\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A-2) DONNEE TRAIN   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing data:\n",
      " counter_id                   0\n",
      "counter_name                 0\n",
      "site_id                      0\n",
      "site_name                    0\n",
      "bike_count                   0\n",
      "date                         0\n",
      "counter_installation_date    0\n",
      "coordinates                  0\n",
      "counter_technical_id         0\n",
      "latitude                     0\n",
      "longitude                    0\n",
      "log_bike_count               0\n",
      "dtype: int64\n",
      "Number of duplicates before removal: 0\n",
      "Number of duplicates after removal: 0\n"
     ]
    }
   ],
   "source": [
    "#Cleaning\n",
    "\n",
    "# 1. Handle Missing Data\n",
    "# Check for missing values\n",
    "missing_data = data.isnull().sum()\n",
    "print(\"Missing data:\\n\", missing_data)\n",
    "\n",
    "# Drop rows with missing values in crucial columns\n",
    "data = data.dropna(subset=['bike_count', 'date', 'latitude', 'longitude'])\n",
    "\n",
    "# 3. Remove Duplicates\n",
    "print(\"Number of duplicates before removal:\", data.duplicated().sum())\n",
    "data = data.drop_duplicates(subset=['counter_id', 'date'])\n",
    "print(\"Number of duplicates after removal:\", data.duplicated().sum())\n",
    "\n",
    "# 4. Handle Outliers (check negative values in bike_count)\n",
    "if (data['bike_count'] < 0).any():\n",
    "    print(\"Negative bike counts detected.\")\n",
    "    data['bike_count'] = data['bike_count'].clip(lower=0)  # Replace negative values with 0\n",
    "\n",
    "# 5. Validate Geographic Coordinates\n",
    "# Check for invalid latitude and longitude values\n",
    "invalid_coords = data[(data['latitude'] < -90) | (data['latitude'] > 90) |\n",
    "                      (data['longitude'] < -180) | (data['longitude'] > 180)]\n",
    "if not invalid_coords.empty:\n",
    "    print(f\"Invalid coordinates detected:\\n{invalid_coords}\")\n",
    "    data = data.dropna(subset=['latitude', 'longitude'])  # Optionally drop invalid coordinates\n",
    "\n",
    "# 6. Clean Text Columns (remove extra spaces, convert to lowercase)\n",
    "text_columns = ['counter_name', 'site_name', 'counter_technical_id']\n",
    "for col in text_columns:\n",
    "    data[col] = data[col].str.strip().str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "counter_id                     56\n",
       "counter_name                   56\n",
       "site_id                        30\n",
       "site_name                      30\n",
       "bike_count                    998\n",
       "date                         8974\n",
       "counter_installation_date      22\n",
       "coordinates                    30\n",
       "counter_technical_id           30\n",
       "latitude                       30\n",
       "longitude                      30\n",
       "log_bike_count                998\n",
       "dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7. modalités uniques\n",
    "data.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>counter_id</th>\n",
       "      <th>counter_name</th>\n",
       "      <th>site_id</th>\n",
       "      <th>site_name</th>\n",
       "      <th>bike_count</th>\n",
       "      <th>date</th>\n",
       "      <th>counter_installation_date</th>\n",
       "      <th>coordinates</th>\n",
       "      <th>counter_technical_id</th>\n",
       "      <th>latitude</th>\n",
       "      <th>...</th>\n",
       "      <th>date_only</th>\n",
       "      <th>hour_only</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>is_rush_hour</th>\n",
       "      <th>season</th>\n",
       "      <th>vacation</th>\n",
       "      <th>is_public_holiday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>48321</th>\n",
       "      <td>100007049-102007049</td>\n",
       "      <td>28 boulevard diderot e-o</td>\n",
       "      <td>100007049</td>\n",
       "      <td>28 boulevard diderot</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2020-09-01 02:00:00</td>\n",
       "      <td>2013-01-18</td>\n",
       "      <td>48.846028,2.375429</td>\n",
       "      <td>y2h15027244</td>\n",
       "      <td>48.846028</td>\n",
       "      <td>...</td>\n",
       "      <td>2020-09-01</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>2020</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>autumn</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48324</th>\n",
       "      <td>100007049-102007049</td>\n",
       "      <td>28 boulevard diderot e-o</td>\n",
       "      <td>100007049</td>\n",
       "      <td>28 boulevard diderot</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2020-09-01 03:00:00</td>\n",
       "      <td>2013-01-18</td>\n",
       "      <td>48.846028,2.375429</td>\n",
       "      <td>y2h15027244</td>\n",
       "      <td>48.846028</td>\n",
       "      <td>...</td>\n",
       "      <td>2020-09-01</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>2020</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>autumn</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48327</th>\n",
       "      <td>100007049-102007049</td>\n",
       "      <td>28 boulevard diderot e-o</td>\n",
       "      <td>100007049</td>\n",
       "      <td>28 boulevard diderot</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2020-09-01 04:00:00</td>\n",
       "      <td>2013-01-18</td>\n",
       "      <td>48.846028,2.375429</td>\n",
       "      <td>y2h15027244</td>\n",
       "      <td>48.846028</td>\n",
       "      <td>...</td>\n",
       "      <td>2020-09-01</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>2020</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>autumn</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48330</th>\n",
       "      <td>100007049-102007049</td>\n",
       "      <td>28 boulevard diderot e-o</td>\n",
       "      <td>100007049</td>\n",
       "      <td>28 boulevard diderot</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2020-09-01 15:00:00</td>\n",
       "      <td>2013-01-18</td>\n",
       "      <td>48.846028,2.375429</td>\n",
       "      <td>y2h15027244</td>\n",
       "      <td>48.846028</td>\n",
       "      <td>...</td>\n",
       "      <td>2020-09-01</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>2020</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>autumn</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48333</th>\n",
       "      <td>100007049-102007049</td>\n",
       "      <td>28 boulevard diderot e-o</td>\n",
       "      <td>100007049</td>\n",
       "      <td>28 boulevard diderot</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2020-09-01 18:00:00</td>\n",
       "      <td>2013-01-18</td>\n",
       "      <td>48.846028,2.375429</td>\n",
       "      <td>y2h15027244</td>\n",
       "      <td>48.846028</td>\n",
       "      <td>...</td>\n",
       "      <td>2020-09-01</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>2020</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>autumn</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                counter_id              counter_name    site_id  \\\n",
       "48321  100007049-102007049  28 boulevard diderot e-o  100007049   \n",
       "48324  100007049-102007049  28 boulevard diderot e-o  100007049   \n",
       "48327  100007049-102007049  28 boulevard diderot e-o  100007049   \n",
       "48330  100007049-102007049  28 boulevard diderot e-o  100007049   \n",
       "48333  100007049-102007049  28 boulevard diderot e-o  100007049   \n",
       "\n",
       "                  site_name  bike_count                date  \\\n",
       "48321  28 boulevard diderot         0.0 2020-09-01 02:00:00   \n",
       "48324  28 boulevard diderot         1.0 2020-09-01 03:00:00   \n",
       "48327  28 boulevard diderot         0.0 2020-09-01 04:00:00   \n",
       "48330  28 boulevard diderot         4.0 2020-09-01 15:00:00   \n",
       "48333  28 boulevard diderot         9.0 2020-09-01 18:00:00   \n",
       "\n",
       "      counter_installation_date         coordinates counter_technical_id  \\\n",
       "48321                2013-01-18  48.846028,2.375429          y2h15027244   \n",
       "48324                2013-01-18  48.846028,2.375429          y2h15027244   \n",
       "48327                2013-01-18  48.846028,2.375429          y2h15027244   \n",
       "48330                2013-01-18  48.846028,2.375429          y2h15027244   \n",
       "48333                2013-01-18  48.846028,2.375429          y2h15027244   \n",
       "\n",
       "        latitude  ...   date_only  hour_only day_of_week  month  year  \\\n",
       "48321  48.846028  ...  2020-09-01          2           1      9  2020   \n",
       "48324  48.846028  ...  2020-09-01          3           1      9  2020   \n",
       "48327  48.846028  ...  2020-09-01          4           1      9  2020   \n",
       "48330  48.846028  ...  2020-09-01         15           1      9  2020   \n",
       "48333  48.846028  ...  2020-09-01         18           1      9  2020   \n",
       "\n",
       "       is_weekend  is_rush_hour  season  vacation is_public_holiday  \n",
       "48321           0             0  autumn         0                 0  \n",
       "48324           0             0  autumn         0                 0  \n",
       "48327           0             0  autumn         0                 0  \n",
       "48330           0             0  autumn         0                 0  \n",
       "48333           0             1  autumn         0                 0  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Convert `date` column to datetime type\n",
    "data['date'] = pd.to_datetime(data['date'])\n",
    "\n",
    "# 2. Extract temporal components\n",
    "data['date_only'] = data['date'].dt.date\n",
    "data['hour_only'] = data['date'].dt.hour\n",
    "data['day_of_week'] = data['date'].dt.dayofweek\n",
    "data['month'] = data['date'].dt.month\n",
    "data['year'] = data['date'].dt.year\n",
    "\n",
    "# 3. Add temporal indicators\n",
    "data['is_weekend'] = data['day_of_week'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "data['is_rush_hour'] = data['hour_only'].apply(lambda x: 1 if (7 <= x <= 9 or 17 <= x <= 19) else 0)\n",
    "data['season'] = data['month'].apply(lambda x: 'winter' if x in [12, 1, 2] \\\n",
    "                                      else 'spring' if x in [3, 4, 5] \\\n",
    "                                      else 'summer' if x in [6, 7, 8] \\\n",
    "                                      else 'autumn')\n",
    "\n",
    "# 4. Add holiday indicators\n",
    "def vacation(date):\n",
    "    summer_vacation = (date.month == 7 or date.month == 8)\n",
    "    christmas_vacation = (date.month == 12 and date.day >= 20)\n",
    "    return 1 if (summer_vacation or christmas_vacation) else 0\n",
    "\n",
    "data['vacation'] = data['date'].apply(vacation)\n",
    "\n",
    "# 5. Add public holidays using the holidays library\n",
    "fr_holidays = holidays.France(years=data['year'].unique().tolist())\n",
    "def is_public_holiday(date):\n",
    "    return 1 if date in fr_holidays else 0\n",
    "\n",
    "data['is_public_holiday'] = data['date'].apply(is_public_holiday)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 496827 entries, 48321 to 929187\n",
      "Data columns (total 22 columns):\n",
      " #   Column                     Non-Null Count   Dtype         \n",
      "---  ------                     --------------   -----         \n",
      " 0   counter_id                 496827 non-null  category      \n",
      " 1   counter_name               496827 non-null  object        \n",
      " 2   site_id                    496827 non-null  int64         \n",
      " 3   site_name                  496827 non-null  object        \n",
      " 4   bike_count                 496827 non-null  float64       \n",
      " 5   date                       496827 non-null  datetime64[us]\n",
      " 6   counter_installation_date  496827 non-null  datetime64[us]\n",
      " 7   coordinates                496827 non-null  category      \n",
      " 8   counter_technical_id       496827 non-null  object        \n",
      " 9   latitude                   496827 non-null  float64       \n",
      " 10  longitude                  496827 non-null  float64       \n",
      " 11  log_bike_count             496827 non-null  float64       \n",
      " 12  date_only                  496827 non-null  object        \n",
      " 13  hour_only                  496827 non-null  int32         \n",
      " 14  day_of_week                496827 non-null  int32         \n",
      " 15  month                      496827 non-null  int32         \n",
      " 16  year                       496827 non-null  int32         \n",
      " 17  is_weekend                 496827 non-null  int64         \n",
      " 18  is_rush_hour               496827 non-null  int64         \n",
      " 19  season                     496827 non-null  object        \n",
      " 20  vacation                   496827 non-null  int64         \n",
      " 21  is_public_holiday          496827 non-null  int64         \n",
      "dtypes: category(2), datetime64[us](2), float64(4), int32(4), int64(5), object(5)\n",
      "memory usage: 73.0+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create features (X) and response variable (y)\n",
    "X = data.drop(columns=['bike_count', 'log_bike_count', 'counter_id', 'site_id', 'coordinates', 'counter_technical_id', 'date_only']) \n",
    "y = data['log_bike_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test split\n",
    "data_train, data_test, target_train, target_test = train_test_split(\n",
    "    X, y, random_state=2408)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B) MODELES\n",
    "\n",
    "Modèles testés:  \n",
    "- Decision tree regressor\n",
    "- Random forest regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from dirty_cat import TableVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STEP 1: Creer toutes les combinaisons possibles de features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B-1) Decision tree regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only numerical features\n",
    "data_train_num = data_train.select_dtypes(include=\"number\")\n",
    "data_train_cat = data_train.select_dtypes(include=\"object\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Audrey/bike_counters-AM-/env/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:425: FitFailedWarning: \n",
      "200 fits failed out of a total of 400.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "200 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/Audrey/bike_counters-AM-/env/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/Audrey/bike_counters-AM-/env/lib/python3.8/site-packages/sklearn/base.py\", line 1145, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"/Users/Audrey/bike_counters-AM-/env/lib/python3.8/site-packages/sklearn/base.py\", line 638, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"/Users/Audrey/bike_counters-AM-/env/lib/python3.8/site-packages/sklearn/utils/_param_validation.py\", line 96, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'min_samples_split' parameter of DecisionTreeRegressor must be an int in the range [2, inf) or a float in the range (0.0, 1.0]. Got 1 instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/Audrey/bike_counters-AM-/env/lib/python3.8/site-packages/sklearn/model_selection/_search.py:979: UserWarning: One or more of the test scores are non-finite: [       nan 0.78124759        nan 0.78125779        nan 0.78126427\n",
      "        nan 0.78126091        nan 0.78124777        nan 0.78124341\n",
      "        nan 0.78124015        nan 0.78124538        nan 0.80062473\n",
      "        nan 0.80064664        nan 0.80066706        nan 0.80066777\n",
      "        nan 0.80064486        nan 0.8006368         nan 0.80063579\n",
      "        nan 0.80063147        nan 0.81935512        nan 0.81935955\n",
      "        nan 0.81940686        nan 0.81942847        nan 0.81940483\n",
      "        nan 0.8194086         nan 0.81941042        nan 0.81942622\n",
      "        nan 0.83176078        nan 0.83179204        nan 0.83187304\n",
      "        nan 0.83193844        nan 0.83193792        nan 0.83190526\n",
      "        nan 0.83188812        nan 0.83188923        nan 0.84103494\n",
      "        nan 0.84114834        nan 0.84125651        nan 0.84138135\n",
      "        nan 0.84140288        nan 0.84141943        nan 0.84140797\n",
      "        nan 0.84140631]\n",
      "  warnings.warn(\n",
      "/Users/Audrey/bike_counters-AM-/env/lib/python3.8/site-packages/sklearn/model_selection/_search.py:979: UserWarning: One or more of the train scores are non-finite: [       nan 0.78427633        nan 0.78426259        nan 0.78424876\n",
      "        nan 0.78423015        nan 0.78421841        nan 0.78420439\n",
      "        nan 0.78418885        nan 0.78417629        nan 0.80509026\n",
      "        nan 0.80505091        nan 0.80501391        nan 0.80497491\n",
      "        nan 0.80493612        nan 0.80489303        nan 0.80486895\n",
      "        nan 0.80483218        nan 0.82570544        nan 0.82561382\n",
      "        nan 0.8255402         nan 0.82545739        nan 0.82535725\n",
      "        nan 0.82527366        nan 0.82520253        nan 0.82513437\n",
      "        nan 0.84068808        nan 0.84053041        nan 0.84036385\n",
      "        nan 0.84018711        nan 0.83999305        nan 0.83981605\n",
      "        nan 0.83966519        nan 0.83952021        nan 0.85280793\n",
      "        nan 0.85254787        nan 0.85226466        nan 0.85197507\n",
      "        nan 0.85168219        nan 0.85140728        nan 0.85118144\n",
      "        nan 0.85097565]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 2 13\n"
     ]
    }
   ],
   "source": [
    "# 1 - FIND BEST PARAMETERS\n",
    "\n",
    "# store parameters to test in a dictionnary\n",
    "param_grid = {'max_depth': range(10, 15), \n",
    "    'min_samples_split': range(1,3),  \n",
    "    'min_samples_leaf': range(8,16)} \n",
    "\n",
    "# initialize the decision tree regressor\n",
    "dtr_model = DecisionTreeRegressor(random_state=2408)\n",
    "\n",
    "# initialize GridSearchCV\n",
    "grid_search = GridSearchCV(dtr_model, param_grid=param_grid, n_jobs=4, cv=5, return_train_score=True)\n",
    "\n",
    "# fit GridSearchCV\n",
    "grid_search.fit(data_train_num, target_train) # REGLER CE PB, TU PRENDS QUE LES NUMERIQUES\n",
    "\n",
    "# get the best max_depth\n",
    "best_max_depth = grid_search.best_params_['max_depth']\n",
    "best_split = grid_search.best_params_['min_samples_split']\n",
    "best_leaf = grid_search.best_params_['min_samples_leaf']\n",
    "\n",
    "print(best_max_depth, best_split, best_leaf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE without scaler: 0.608236986170773\n"
     ]
    }
   ],
   "source": [
    "# DEFINE PIPELINES and test RMSE - WITHOUT SCALER\n",
    "\n",
    "# Pipeline\n",
    "pipeline_no_scaler_dtr = make_pipeline(TableVectorizer(),\n",
    "    DecisionTreeRegressor(max_depth=best_max_depth,min_samples_split=best_split,min_samples_leaf=best_leaf)\n",
    ")\n",
    "\n",
    "# RMSE Without scaler\n",
    "pipeline_no_scaler_dtr.fit(data_train, target_train)\n",
    "target_pred_no_scaler = pipeline_no_scaler_dtr.predict(data_test)\n",
    "rmse_no_scaler = mean_squared_error(target_test, target_pred_no_scaler, squared=False)\n",
    "print(f\"RMSE without scaler: {rmse_no_scaler}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE with scaler: 0.6058565069881058\n"
     ]
    }
   ],
   "source": [
    "# DEFINE PIPELINES and test RMSE - WITH SCALER\n",
    "\n",
    "# split numerical and categorical\n",
    "dfcat= X.select_dtypes(include=\"object\").columns\n",
    "dfnum= X.select_dtypes(include=\"number\").columns\n",
    "\n",
    "\n",
    "# define ColumnTransformer\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('table_vectorizer', TableVectorizer(),\n",
    "     dfcat),\n",
    "    ('standard-scaler', StandardScaler(), dfnum)\n",
    "])\n",
    "\n",
    "# with scaler\n",
    "pipeline_scaler_dtr = make_pipeline(\n",
    "    preprocessor,\n",
    "    DecisionTreeRegressor(max_depth=best_max_depth, min_samples_split=best_split, min_samples_leaf=best_leaf)\n",
    ")\n",
    "\n",
    "# RMSE with scaler\n",
    "pipeline_scaler_dtr.fit(data_train, target_train)\n",
    "target_pred_scaler = pipeline_scaler_dtr.predict(data_test)\n",
    "rmse_scaler = mean_squared_error(target_test, target_pred_scaler, squared=False)\n",
    "print(f\"RMSE with scaler: {rmse_scaler}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "APPLY ON TEST DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m test \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_parquet(Path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinal_test.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m# import test \u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "test = pd.read_parquet(Path(\"data\") / \"final_test.parquet\") # import test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
